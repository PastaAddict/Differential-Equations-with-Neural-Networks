{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import pyDOE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN,self).__init__()\n",
    "        #layer definitions\n",
    "        self.FC1 = nn.Linear(2,50)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.FC2 = nn.Linear(50,50)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.FC3 = nn.Linear(50,1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        y  = torch.cat((x,t),1)\n",
    "        x1 = self.FC1(y)\n",
    "        x2 = self.act1(x1)\n",
    "        x3 = self.FC2(x2)\n",
    "        x4 = self.act2(x3)\n",
    "        x5 = self.FC3(x4)\n",
    "        return x5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "design = pyDOE.lhs(2, samples = 1000)\n",
    "x = design[:,0] #1st variable\n",
    "t = design[:,1] #2nd variable\n",
    "f = -2*(np.pi**2)*np.sin(np.pi*x)*np.sin(np.pi*t)\n",
    "\n",
    "#boundary\n",
    "x_b = np.linspace(0,1,10)\n",
    "t_b = np.linspace(0,1,10)\n",
    "\n",
    "t_0 = np.zeros(10)\n",
    "t_1 = np.zeros(10)+1\n",
    "\n",
    "x_0 = np.zeros(10)\n",
    "x_1 = np.zeros(10)+1\n",
    "\n",
    "u_bc = np.zeros(40)\n",
    "\n",
    "#to tensor\n",
    "\n",
    "x = x.reshape((-1, 1))\n",
    "x = torch.tensor(x).type(torch.FloatTensor)\n",
    "t = t.reshape((-1, 1))\n",
    "t = torch.tensor(t).type(torch.FloatTensor)\n",
    "f = f.reshape((-1, 1))\n",
    "f = torch.tensor(f).type(torch.FloatTensor)\n",
    "\n",
    "x_b = x_b.reshape((-1, 1))\n",
    "x_b = torch.tensor(x_b).type(torch.FloatTensor)\n",
    "x_0 = x_0.reshape((-1, 1))\n",
    "x_0 = torch.tensor(x_0).type(torch.FloatTensor)\n",
    "x_1 = x_1.reshape((-1, 1))\n",
    "x_1 = torch.tensor(x_1).type(torch.FloatTensor)\n",
    "\n",
    "t_b = t_b.reshape((-1, 1))\n",
    "t_b = torch.tensor(t_b).type(torch.FloatTensor)\n",
    "t_0 = t_0.reshape((-1, 1))\n",
    "t_0 = torch.tensor(t_0).type(torch.FloatTensor)\n",
    "t_1 = t_1.reshape((-1, 1))\n",
    "t_1 = torch.tensor(t_1).type(torch.FloatTensor)\n",
    "\n",
    "x_bc = torch.cat((x_b,x_b,x_0,x_1),0)\n",
    "t_bc = torch.cat((t_0,t_1,t_b,t_b),0)\n",
    "u_bc = u_bc.reshape((-1, 1))\n",
    "u_bc = torch.tensor(u_bc).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = PINN()\n",
    "#pn.apply(init_weights)\n",
    "\n",
    "MAX_EPOCHS = 20000\n",
    "LRATE = 1e-4\n",
    "\n",
    "#L=torch.tensor(1).type(torch.FloatTensor)\n",
    "#L.requires_grad = True\n",
    "\n",
    "#Use Adam for training\n",
    "optimizer = torch.optim.Adam(pn.parameters(), lr=LRATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss_history_u = []\n",
    "loss_history_f = []\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, MSE_u: 0.0906114, MSE_f: 94.7265625, MSE: 94.8171768, l: 1.00\n",
      "Epoch: 200, MSE_u: 0.6505258, MSE_f: 86.6493378, MSE: 87.2998657, l: 1.00\n",
      "Epoch: 300, MSE_u: 1.5927211, MSE_f: 74.4440002, MSE: 76.0367203, l: 1.00\n",
      "Epoch: 400, MSE_u: 2.7054064, MSE_f: 57.4994621, MSE: 60.2048683, l: 1.00\n",
      "Epoch: 500, MSE_u: 3.6603551, MSE_f: 38.2116470, MSE: 41.8720016, l: 1.00\n",
      "Epoch: 600, MSE_u: 4.0609617, MSE_f: 21.8179779, MSE: 25.8789406, l: 1.00\n",
      "Epoch: 700, MSE_u: 3.7320721, MSE_f: 11.7732248, MSE: 15.5052967, l: 1.00\n",
      "Epoch: 800, MSE_u: 2.9882801, MSE_f: 6.7681675, MSE: 9.7564478, l: 1.00\n",
      "Epoch: 900, MSE_u: 2.2975504, MSE_f: 4.3827777, MSE: 6.6803284, l: 1.00\n",
      "Epoch: 1000, MSE_u: 1.7751538, MSE_f: 3.3185508, MSE: 5.0937047, l: 1.00\n",
      "Epoch: 1100, MSE_u: 1.3860230, MSE_f: 2.7242410, MSE: 4.1102638, l: 1.00\n",
      "Epoch: 1200, MSE_u: 1.0829886, MSE_f: 2.2390652, MSE: 3.3220539, l: 1.00\n",
      "Epoch: 1300, MSE_u: 0.8363479, MSE_f: 1.8084964, MSE: 2.6448443, l: 1.00\n",
      "Epoch: 1400, MSE_u: 0.6326541, MSE_f: 1.4464290, MSE: 2.0790830, l: 1.00\n",
      "Epoch: 1500, MSE_u: 0.4664704, MSE_f: 1.1597792, MSE: 1.6262496, l: 1.00\n",
      "Epoch: 1600, MSE_u: 0.3350633, MSE_f: 0.9415533, MSE: 1.2766166, l: 1.00\n",
      "Epoch: 1700, MSE_u: 0.2353177, MSE_f: 0.7795430, MSE: 1.0148606, l: 1.00\n",
      "Epoch: 1800, MSE_u: 0.1627683, MSE_f: 0.6608024, MSE: 0.8235707, l: 1.00\n",
      "Epoch: 1900, MSE_u: 0.1120843, MSE_f: 0.5731099, MSE: 0.6851942, l: 1.00\n",
      "Epoch: 2000, MSE_u: 0.0779529, MSE_f: 0.5062771, MSE: 0.5842300, l: 1.00\n",
      "Epoch: 2100, MSE_u: 0.0557405, MSE_f: 0.4526768, MSE: 0.5084173, l: 1.00\n",
      "Epoch: 2200, MSE_u: 0.0417853, MSE_f: 0.4069358, MSE: 0.4487211, l: 1.00\n",
      "Epoch: 2300, MSE_u: 0.0333932, MSE_f: 0.3655186, MSE: 0.3989118, l: 1.00\n",
      "Epoch: 2400, MSE_u: 0.0286797, MSE_f: 0.3264359, MSE: 0.3551156, l: 1.00\n",
      "Epoch: 2500, MSE_u: 0.0263751, MSE_f: 0.2889732, MSE: 0.3153484, l: 1.00\n",
      "Epoch: 2600, MSE_u: 0.0256397, MSE_f: 0.2533854, MSE: 0.2790251, l: 1.00\n",
      "Epoch: 2700, MSE_u: 0.0259002, MSE_f: 0.2204933, MSE: 0.2463935, l: 1.00\n",
      "Epoch: 2800, MSE_u: 0.0267279, MSE_f: 0.1911761, MSE: 0.2179039, l: 1.00\n",
      "Epoch: 2900, MSE_u: 0.0277795, MSE_f: 0.1659386, MSE: 0.1937181, l: 1.00\n",
      "Epoch: 3000, MSE_u: 0.0287950, MSE_f: 0.1447578, MSE: 0.1735528, l: 1.00\n",
      "Epoch: 3100, MSE_u: 0.0296080, MSE_f: 0.1272114, MSE: 0.1568194, l: 1.00\n",
      "Epoch: 3200, MSE_u: 0.0301348, MSE_f: 0.1127125, MSE: 0.1428473, l: 1.00\n",
      "Epoch: 3300, MSE_u: 0.0303511, MSE_f: 0.1006799, MSE: 0.1310309, l: 1.00\n",
      "Epoch: 3400, MSE_u: 0.0302703, MSE_f: 0.0906156, MSE: 0.1208859, l: 1.00\n",
      "Epoch: 3500, MSE_u: 0.0299292, MSE_f: 0.0821216, MSE: 0.1120508, l: 1.00\n",
      "Epoch: 3600, MSE_u: 0.0293765, MSE_f: 0.0748929, MSE: 0.1042695, l: 1.00\n",
      "Epoch: 3700, MSE_u: 0.0286646, MSE_f: 0.0687008, MSE: 0.0973654, l: 1.00\n",
      "Epoch: 3800, MSE_u: 0.0278433, MSE_f: 0.0633723, MSE: 0.0912157, l: 1.00\n",
      "Epoch: 3900, MSE_u: 0.0269564, MSE_f: 0.0587727, MSE: 0.0857291, l: 1.00\n",
      "Epoch: 4000, MSE_u: 0.0260401, MSE_f: 0.0547894, MSE: 0.0808295, l: 1.00\n",
      "Epoch: 4100, MSE_u: 0.0251222, MSE_f: 0.0513232, MSE: 0.0764454, l: 1.00\n",
      "Epoch: 4200, MSE_u: 0.0242226, MSE_f: 0.0482829, MSE: 0.0725054, l: 1.00\n",
      "Epoch: 4300, MSE_u: 0.0233534, MSE_f: 0.0455838, MSE: 0.0689372, l: 1.00\n",
      "Epoch: 4400, MSE_u: 0.0225202, MSE_f: 0.0431505, MSE: 0.0656707, l: 1.00\n",
      "Epoch: 4500, MSE_u: 0.0217234, MSE_f: 0.0409201, MSE: 0.0626435, l: 1.00\n",
      "Epoch: 4600, MSE_u: 0.0209625, MSE_f: 0.0388472, MSE: 0.0598097, l: 1.00\n",
      "Epoch: 4700, MSE_u: 0.0202410, MSE_f: 0.0369060, MSE: 0.0571470, l: 1.00\n",
      "Epoch: 4800, MSE_u: 0.0195707, MSE_f: 0.0350842, MSE: 0.0546550, l: 1.00\n",
      "Epoch: 4900, MSE_u: 0.0189691, MSE_f: 0.0333703, MSE: 0.0523395, l: 1.00\n",
      "Epoch: 5000, MSE_u: 0.0184494, MSE_f: 0.0317435, MSE: 0.0501930, l: 1.00\n",
      "Epoch: 5100, MSE_u: 0.0180132, MSE_f: 0.0301781, MSE: 0.0481913, l: 1.00\n",
      "Epoch: 5200, MSE_u: 0.0176510, MSE_f: 0.0286523, MSE: 0.0463033, l: 1.00\n",
      "Epoch: 5300, MSE_u: 0.0173486, MSE_f: 0.0271535, MSE: 0.0445021, l: 1.00\n",
      "Epoch: 5400, MSE_u: 0.0170918, MSE_f: 0.0256767, MSE: 0.0427685, l: 1.00\n",
      "Epoch: 5500, MSE_u: 0.0168693, MSE_f: 0.0242218, MSE: 0.0410911, l: 1.00\n",
      "Epoch: 5600, MSE_u: 0.0166727, MSE_f: 0.0227917, MSE: 0.0394643, l: 1.00\n",
      "Epoch: 5700, MSE_u: 0.0164957, MSE_f: 0.0213920, MSE: 0.0378877, l: 1.00\n",
      "Epoch: 5800, MSE_u: 0.0163336, MSE_f: 0.0200311, MSE: 0.0363647, l: 1.00\n",
      "Epoch: 5900, MSE_u: 0.0161829, MSE_f: 0.0187187, MSE: 0.0349016, l: 1.00\n",
      "Epoch: 6000, MSE_u: 0.0160405, MSE_f: 0.0174655, MSE: 0.0335060, l: 1.00\n",
      "Epoch: 6100, MSE_u: 0.0159036, MSE_f: 0.0162806, MSE: 0.0321841, l: 1.00\n",
      "Epoch: 6200, MSE_u: 0.0157695, MSE_f: 0.0151698, MSE: 0.0309393, l: 1.00\n",
      "Epoch: 6300, MSE_u: 0.0156355, MSE_f: 0.0141349, MSE: 0.0297704, l: 1.00\n",
      "Epoch: 6400, MSE_u: 0.0154987, MSE_f: 0.0131733, MSE: 0.0286720, l: 1.00\n",
      "Epoch: 6500, MSE_u: 0.0153564, MSE_f: 0.0122794, MSE: 0.0276358, l: 1.00\n",
      "Epoch: 6600, MSE_u: 0.0152056, MSE_f: 0.0114457, MSE: 0.0266513, l: 1.00\n",
      "Epoch: 6700, MSE_u: 0.0150440, MSE_f: 0.0106644, MSE: 0.0257084, l: 1.00\n",
      "Epoch: 6800, MSE_u: 0.0148696, MSE_f: 0.0099285, MSE: 0.0247981, l: 1.00\n",
      "Epoch: 6900, MSE_u: 0.0146809, MSE_f: 0.0092327, MSE: 0.0239136, l: 1.00\n",
      "Epoch: 7000, MSE_u: 0.0144777, MSE_f: 0.0085735, MSE: 0.0230512, l: 1.00\n",
      "Epoch: 7100, MSE_u: 0.0142604, MSE_f: 0.0079497, MSE: 0.0222101, l: 1.00\n",
      "Epoch: 7200, MSE_u: 0.0140302, MSE_f: 0.0073617, MSE: 0.0213919, l: 1.00\n",
      "Epoch: 7300, MSE_u: 0.0137892, MSE_f: 0.0068104, MSE: 0.0205996, l: 1.00\n",
      "Epoch: 7400, MSE_u: 0.0135394, MSE_f: 0.0062967, MSE: 0.0198361, l: 1.00\n",
      "Epoch: 7500, MSE_u: 0.0132831, MSE_f: 0.0058205, MSE: 0.0191036, l: 1.00\n",
      "Epoch: 7600, MSE_u: 0.0130221, MSE_f: 0.0053809, MSE: 0.0184030, l: 1.00\n",
      "Epoch: 7700, MSE_u: 0.0127577, MSE_f: 0.0049765, MSE: 0.0177343, l: 1.00\n",
      "Epoch: 7800, MSE_u: 0.0124914, MSE_f: 0.0046059, MSE: 0.0170973, l: 1.00\n",
      "Epoch: 7900, MSE_u: 0.0122240, MSE_f: 0.0042680, MSE: 0.0164921, l: 1.00\n",
      "Epoch: 8000, MSE_u: 0.0119569, MSE_f: 0.0039624, MSE: 0.0159193, l: 1.00\n",
      "Epoch: 8100, MSE_u: 0.0116912, MSE_f: 0.0036883, MSE: 0.0153794, l: 1.00\n",
      "Epoch: 8200, MSE_u: 0.0114279, MSE_f: 0.0034447, MSE: 0.0148726, l: 1.00\n",
      "Epoch: 8300, MSE_u: 0.0111678, MSE_f: 0.0032301, MSE: 0.0143979, l: 1.00\n",
      "Epoch: 8400, MSE_u: 0.0109115, MSE_f: 0.0030417, MSE: 0.0139532, l: 1.00\n",
      "Epoch: 8500, MSE_u: 0.0106590, MSE_f: 0.0028762, MSE: 0.0135352, l: 1.00\n",
      "Epoch: 8600, MSE_u: 0.0104097, MSE_f: 0.0027299, MSE: 0.0131396, l: 1.00\n",
      "Epoch: 8700, MSE_u: 0.0101628, MSE_f: 0.0025989, MSE: 0.0127617, l: 1.00\n",
      "Epoch: 8800, MSE_u: 0.0099170, MSE_f: 0.0024797, MSE: 0.0123967, l: 1.00\n",
      "Epoch: 8900, MSE_u: 0.0096707, MSE_f: 0.0023693, MSE: 0.0120400, l: 1.00\n",
      "Epoch: 9000, MSE_u: 0.0094224, MSE_f: 0.0022651, MSE: 0.0116876, l: 1.00\n",
      "Epoch: 9100, MSE_u: 0.0091706, MSE_f: 0.0021654, MSE: 0.0113360, l: 1.00\n",
      "Epoch: 9200, MSE_u: 0.0089138, MSE_f: 0.0020687, MSE: 0.0109825, l: 1.00\n",
      "Epoch: 9300, MSE_u: 0.0086508, MSE_f: 0.0019741, MSE: 0.0106249, l: 1.00\n",
      "Epoch: 9400, MSE_u: 0.0083807, MSE_f: 0.0018811, MSE: 0.0102618, l: 1.00\n",
      "Epoch: 9500, MSE_u: 0.0081029, MSE_f: 0.0017896, MSE: 0.0098925, l: 1.00\n",
      "Epoch: 9600, MSE_u: 0.0078173, MSE_f: 0.0017000, MSE: 0.0095173, l: 1.00\n",
      "Epoch: 9700, MSE_u: 0.0075245, MSE_f: 0.0016132, MSE: 0.0091377, l: 1.00\n",
      "Epoch: 9800, MSE_u: 0.0072258, MSE_f: 0.0015303, MSE: 0.0087561, l: 1.00\n",
      "Epoch: 9900, MSE_u: 0.0069233, MSE_f: 0.0014528, MSE: 0.0083761, l: 1.00\n",
      "Epoch: 10000, MSE_u: 0.0066195, MSE_f: 0.0013821, MSE: 0.0080016, l: 1.00\n",
      "Epoch: 10100, MSE_u: 0.0063172, MSE_f: 0.0013192, MSE: 0.0076364, l: 1.00\n",
      "Epoch: 10200, MSE_u: 0.0060189, MSE_f: 0.0012644, MSE: 0.0072833, l: 1.00\n",
      "Epoch: 10300, MSE_u: 0.0057265, MSE_f: 0.0012173, MSE: 0.0069438, l: 1.00\n",
      "Epoch: 10400, MSE_u: 0.0054413, MSE_f: 0.0011770, MSE: 0.0066183, l: 1.00\n",
      "Epoch: 10500, MSE_u: 0.0051641, MSE_f: 0.0011428, MSE: 0.0063069, l: 1.00\n",
      "Epoch: 10600, MSE_u: 0.0048963, MSE_f: 0.0011121, MSE: 0.0060084, l: 1.00\n",
      "Epoch: 10700, MSE_u: 0.0046375, MSE_f: 0.0010848, MSE: 0.0057223, l: 1.00\n",
      "Epoch: 10800, MSE_u: 0.0043868, MSE_f: 0.0010629, MSE: 0.0054497, l: 1.00\n",
      "Epoch: 10900, MSE_u: 0.0041472, MSE_f: 0.0010343, MSE: 0.0051815, l: 1.00\n",
      "Epoch: 11000, MSE_u: 0.0039158, MSE_f: 0.0010133, MSE: 0.0049292, l: 1.00\n",
      "Epoch: 11100, MSE_u: 0.0036918, MSE_f: 0.0009836, MSE: 0.0046753, l: 1.00\n",
      "Epoch: 11200, MSE_u: 0.0034768, MSE_f: 0.0009568, MSE: 0.0044337, l: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11300, MSE_u: 0.0032697, MSE_f: 0.0009298, MSE: 0.0041995, l: 1.00\n",
      "Epoch: 11400, MSE_u: 0.0030704, MSE_f: 0.0009024, MSE: 0.0039728, l: 1.00\n",
      "Epoch: 11500, MSE_u: 0.0028781, MSE_f: 0.0008747, MSE: 0.0037528, l: 1.00\n",
      "Epoch: 11600, MSE_u: 0.0026940, MSE_f: 0.0008464, MSE: 0.0035404, l: 1.00\n",
      "Epoch: 11700, MSE_u: 0.0025164, MSE_f: 0.0008179, MSE: 0.0033343, l: 1.00\n",
      "Epoch: 11800, MSE_u: 0.0023457, MSE_f: 0.0008007, MSE: 0.0031464, l: 1.00\n",
      "Epoch: 11900, MSE_u: 0.0021841, MSE_f: 0.0007693, MSE: 0.0029534, l: 1.00\n",
      "Epoch: 12000, MSE_u: 0.0020281, MSE_f: 0.0007314, MSE: 0.0027594, l: 1.00\n",
      "Epoch: 12100, MSE_u: 0.0018806, MSE_f: 0.0007020, MSE: 0.0025827, l: 1.00\n",
      "Epoch: 12200, MSE_u: 0.0017400, MSE_f: 0.0006728, MSE: 0.0024128, l: 1.00\n",
      "Epoch: 12300, MSE_u: 0.0016070, MSE_f: 0.0006441, MSE: 0.0022512, l: 1.00\n",
      "Epoch: 12400, MSE_u: 0.0014813, MSE_f: 0.0006236, MSE: 0.0021049, l: 1.00\n",
      "Epoch: 12500, MSE_u: 0.0013617, MSE_f: 0.0005876, MSE: 0.0019492, l: 1.00\n",
      "Epoch: 12600, MSE_u: 0.0012526, MSE_f: 0.0005651, MSE: 0.0018177, l: 1.00\n",
      "Epoch: 12700, MSE_u: 0.0011470, MSE_f: 0.0005315, MSE: 0.0016786, l: 1.00\n",
      "Epoch: 12800, MSE_u: 0.0010499, MSE_f: 0.0005062, MSE: 0.0015562, l: 1.00\n",
      "Epoch: 12900, MSE_u: 0.0009565, MSE_f: 0.0004934, MSE: 0.0014499, l: 1.00\n",
      "Epoch: 13000, MSE_u: 0.0008761, MSE_f: 0.0004540, MSE: 0.0013301, l: 1.00\n",
      "Epoch: 13100, MSE_u: 0.0007991, MSE_f: 0.0004301, MSE: 0.0012292, l: 1.00\n",
      "Epoch: 13200, MSE_u: 0.0007277, MSE_f: 0.0004074, MSE: 0.0011351, l: 1.00\n",
      "Epoch: 13300, MSE_u: 0.0006621, MSE_f: 0.0003857, MSE: 0.0010478, l: 1.00\n",
      "Epoch: 13400, MSE_u: 0.0006019, MSE_f: 0.0003649, MSE: 0.0009668, l: 1.00\n",
      "Epoch: 13500, MSE_u: 0.0005454, MSE_f: 0.0003561, MSE: 0.0009015, l: 1.00\n",
      "Epoch: 13600, MSE_u: 0.0004966, MSE_f: 0.0003273, MSE: 0.0008239, l: 1.00\n",
      "Epoch: 13700, MSE_u: 0.0004507, MSE_f: 0.0003101, MSE: 0.0007608, l: 1.00\n",
      "Epoch: 13800, MSE_u: 0.0004088, MSE_f: 0.0002942, MSE: 0.0007030, l: 1.00\n",
      "Epoch: 13900, MSE_u: 0.0003713, MSE_f: 0.0002795, MSE: 0.0006508, l: 1.00\n",
      "Epoch: 14000, MSE_u: 0.0003367, MSE_f: 0.0002706, MSE: 0.0006074, l: 1.00\n",
      "Epoch: 14100, MSE_u: 0.0003070, MSE_f: 0.0002532, MSE: 0.0005603, l: 1.00\n",
      "Epoch: 14200, MSE_u: 0.0002795, MSE_f: 0.0002417, MSE: 0.0005211, l: 1.00\n",
      "Epoch: 14300, MSE_u: 0.0002548, MSE_f: 0.0002309, MSE: 0.0004857, l: 1.00\n",
      "Epoch: 14400, MSE_u: 0.0002326, MSE_f: 0.0002209, MSE: 0.0004535, l: 1.00\n",
      "Epoch: 14500, MSE_u: 0.0002136, MSE_f: 0.0002234, MSE: 0.0004370, l: 1.00\n",
      "Epoch: 14600, MSE_u: 0.0001951, MSE_f: 0.0002052, MSE: 0.0004003, l: 1.00\n",
      "Epoch: 14700, MSE_u: 0.0001793, MSE_f: 0.0002024, MSE: 0.0003817, l: 1.00\n",
      "Epoch: 14800, MSE_u: 0.0001641, MSE_f: 0.0001961, MSE: 0.0003602, l: 1.00\n",
      "Epoch: 14900, MSE_u: 0.0001521, MSE_f: 0.0001812, MSE: 0.0003333, l: 1.00\n",
      "Epoch: 15000, MSE_u: 0.0001407, MSE_f: 0.0001749, MSE: 0.0003157, l: 1.00\n",
      "Epoch: 15100, MSE_u: 0.0001307, MSE_f: 0.0001691, MSE: 0.0002998, l: 1.00\n",
      "Epoch: 15200, MSE_u: 0.0001206, MSE_f: 0.0001907, MSE: 0.0003113, l: 1.00\n",
      "Epoch: 15300, MSE_u: 0.0001136, MSE_f: 0.0001599, MSE: 0.0002735, l: 1.00\n",
      "Epoch: 15400, MSE_u: 0.0001063, MSE_f: 0.0001538, MSE: 0.0002601, l: 1.00\n",
      "Epoch: 15500, MSE_u: 0.0000994, MSE_f: 0.0001585, MSE: 0.0002578, l: 1.00\n",
      "Epoch: 15600, MSE_u: 0.0000933, MSE_f: 0.0001506, MSE: 0.0002439, l: 1.00\n",
      "Epoch: 15700, MSE_u: 0.0000882, MSE_f: 0.0001407, MSE: 0.0002289, l: 1.00\n",
      "Epoch: 15800, MSE_u: 0.0000834, MSE_f: 0.0001365, MSE: 0.0002198, l: 1.00\n",
      "Epoch: 15900, MSE_u: 0.0000789, MSE_f: 0.0001342, MSE: 0.0002131, l: 1.00\n",
      "Epoch: 16000, MSE_u: 0.0000749, MSE_f: 0.0001375, MSE: 0.0002124, l: 1.00\n",
      "Epoch: 16100, MSE_u: 0.0000713, MSE_f: 0.0001295, MSE: 0.0002009, l: 1.00\n",
      "Epoch: 16200, MSE_u: 0.0000676, MSE_f: 0.0001287, MSE: 0.0001964, l: 1.00\n",
      "Epoch: 16300, MSE_u: 0.0000641, MSE_f: 0.0001240, MSE: 0.0001882, l: 1.00\n",
      "Epoch: 16400, MSE_u: 0.0000615, MSE_f: 0.0001166, MSE: 0.0001781, l: 1.00\n",
      "Epoch: 16500, MSE_u: 0.0000588, MSE_f: 0.0001141, MSE: 0.0001729, l: 1.00\n",
      "Epoch: 16600, MSE_u: 0.0000562, MSE_f: 0.0001112, MSE: 0.0001674, l: 1.00\n",
      "Epoch: 16700, MSE_u: 0.0000541, MSE_f: 0.0001098, MSE: 0.0001639, l: 1.00\n",
      "Epoch: 16800, MSE_u: 0.0000517, MSE_f: 0.0001061, MSE: 0.0001578, l: 1.00\n",
      "Epoch: 16900, MSE_u: 0.0000495, MSE_f: 0.0001048, MSE: 0.0001543, l: 1.00\n",
      "Epoch: 17000, MSE_u: 0.0000479, MSE_f: 0.0001339, MSE: 0.0001818, l: 1.00\n",
      "Epoch: 17100, MSE_u: 0.0000458, MSE_f: 0.0000990, MSE: 0.0001449, l: 1.00\n",
      "Epoch: 17200, MSE_u: 0.0000441, MSE_f: 0.0000969, MSE: 0.0001411, l: 1.00\n",
      "Epoch: 17300, MSE_u: 0.0000426, MSE_f: 0.0000950, MSE: 0.0001376, l: 1.00\n",
      "Epoch: 17400, MSE_u: 0.0000410, MSE_f: 0.0000929, MSE: 0.0001339, l: 1.00\n",
      "Epoch: 17500, MSE_u: 0.0000395, MSE_f: 0.0000909, MSE: 0.0001304, l: 1.00\n",
      "Epoch: 17600, MSE_u: 0.0000381, MSE_f: 0.0000890, MSE: 0.0001271, l: 1.00\n",
      "Epoch: 17700, MSE_u: 0.0000368, MSE_f: 0.0000872, MSE: 0.0001240, l: 1.00\n",
      "Epoch: 17800, MSE_u: 0.0000356, MSE_f: 0.0000854, MSE: 0.0001210, l: 1.00\n",
      "Epoch: 17900, MSE_u: 0.0000343, MSE_f: 0.0000858, MSE: 0.0001201, l: 1.00\n",
      "Epoch: 18000, MSE_u: 0.0000332, MSE_f: 0.0000820, MSE: 0.0001152, l: 1.00\n",
      "Epoch: 18100, MSE_u: 0.0000321, MSE_f: 0.0000803, MSE: 0.0001124, l: 1.00\n",
      "Epoch: 18200, MSE_u: 0.0000308, MSE_f: 0.0001072, MSE: 0.0001380, l: 1.00\n",
      "Epoch: 18300, MSE_u: 0.0000304, MSE_f: 0.0000893, MSE: 0.0001197, l: 1.00\n",
      "Epoch: 18400, MSE_u: 0.0000292, MSE_f: 0.0000770, MSE: 0.0001062, l: 1.00\n",
      "Epoch: 18500, MSE_u: 0.0000280, MSE_f: 0.0000822, MSE: 0.0001102, l: 1.00\n",
      "Epoch: 18600, MSE_u: 0.0000271, MSE_f: 0.0000835, MSE: 0.0001106, l: 1.00\n",
      "Epoch: 18700, MSE_u: 0.0000265, MSE_f: 0.0000716, MSE: 0.0000982, l: 1.00\n",
      "Epoch: 18800, MSE_u: 0.0000257, MSE_f: 0.0000700, MSE: 0.0000956, l: 1.00\n",
      "Epoch: 18900, MSE_u: 0.0000249, MSE_f: 0.0000685, MSE: 0.0000934, l: 1.00\n",
      "Epoch: 19000, MSE_u: 0.0000242, MSE_f: 0.0000677, MSE: 0.0000918, l: 1.00\n",
      "Epoch: 19100, MSE_u: 0.0000234, MSE_f: 0.0000659, MSE: 0.0000893, l: 1.00\n",
      "Epoch: 19200, MSE_u: 0.0000226, MSE_f: 0.0000694, MSE: 0.0000921, l: 1.00\n",
      "Epoch: 19300, MSE_u: 0.0000221, MSE_f: 0.0000639, MSE: 0.0000860, l: 1.00\n",
      "Epoch: 19400, MSE_u: 0.0000213, MSE_f: 0.0000624, MSE: 0.0000838, l: 1.00\n",
      "Epoch: 19500, MSE_u: 0.0000209, MSE_f: 0.0000638, MSE: 0.0000846, l: 1.00\n",
      "Epoch: 19600, MSE_u: 0.0000201, MSE_f: 0.0000616, MSE: 0.0000817, l: 1.00\n",
      "Epoch: 19700, MSE_u: 0.0000196, MSE_f: 0.0000619, MSE: 0.0000815, l: 1.00\n",
      "Epoch: 19800, MSE_u: 0.0000190, MSE_f: 0.0000581, MSE: 0.0000772, l: 1.00\n",
      "Epoch: 19900, MSE_u: 0.0000184, MSE_f: 0.0000568, MSE: 0.0000752, l: 1.00\n",
      "Epoch: 20000, MSE_u: 0.0000180, MSE_f: 0.0000573, MSE: 0.0000753, l: 1.00\n"
     ]
    }
   ],
   "source": [
    "l=1\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    #full batch\n",
    "    #u\n",
    "    upred_bc = pn(x_bc, t_bc)\n",
    "\n",
    "    mse_u = criterion(input=upred_bc, target=u_bc)\n",
    "    loss_history_u.append([epoch, mse_u])\n",
    "\n",
    "    #f\n",
    "    xc = x.clone()\n",
    "    xc.requires_grad = True\n",
    "\n",
    "    tc = t.clone()\n",
    "    tc.requires_grad = True\n",
    "\n",
    "    upred = pn(xc, tc)\n",
    "    upred_x = torch.autograd.grad(upred.sum(),xc,create_graph=True)[0]\n",
    "    upred_xx = torch.autograd.grad(upred_x.sum(),xc,create_graph=True)[0]\n",
    "    upred_t = torch.autograd.grad(upred.sum(),tc,create_graph=True)[0]\n",
    "    upred_tt = torch.autograd.grad(upred_t.sum(),tc,create_graph=True)[0]\n",
    "\n",
    "    mse_f = criterion(input=upred_xx + upred_tt, target = f )\n",
    "    loss_history_f.append([epoch, mse_f])\n",
    "    \n",
    "    ###custom loss\n",
    "#    if epoch%5==0:\n",
    "#        mse_u_grad = torch.autograd.grad(mse_u,pn.parameters(),retain_graph=True)\n",
    "#        mse_f_grad = torch.autograd.grad(mse_f,pn.parameters(),retain_graph=True, allow_unused=True)\n",
    "                \n",
    "#        a = torch.cat([torch.flatten(i) for i in mse_u_grad if i is not None])\n",
    "#        b = torch.cat([torch.flatten(i) for i in mse_f_grad if i is not None])\n",
    "\n",
    "#        l = 0.9*torch.mean(torch.abs(b))/torch.mean(torch.abs(a)) + (1 - 0.9)*l\n",
    "    \n",
    "    ###\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#    loss = l*(mse_u.clone().detach()/(mse_u.clone().detach() + mse_f.clone().detach()))*mse_u + mse_f*(mse_f.clone().detach()/(mse_f.clone().detach() + mse_f.clone().detach()))\n",
    "    loss = mse_u + mse_f\n",
    "    loss_history.append([epoch, loss])\n",
    "    \n",
    "    #optimizer step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "        \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(\"Epoch: {}, MSE_u: {:.7f}, MSE_f: {:.7f}, MSE: {:.7f}, l: {:.2f}\".format((epoch+1), mse_u, mse_f, loss, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Line3DCollection at 0x153b02160d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.linspace(0,1,100)\n",
    "Y = np.linspace(0,1,100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z=np.sin(np.pi*X)*np.sin(np.pi*Y)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_wireframe(X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0,1,100)\n",
    "Y = np.linspace(0,1,100)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "\n",
    "\n",
    "\n",
    "x = X.reshape((-1, 1))\n",
    "x = torch.tensor(x).type(torch.FloatTensor)\n",
    "\n",
    "y = Y.reshape((-1, 1))\n",
    "y = torch.tensor(y).type(torch.FloatTensor)\n",
    "\n",
    "U = pn(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Line3DCollection at 0x153b69887f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_wireframe(X, Y,Z - U.detach().numpy().reshape(100,100))\n",
    "#ax.plot_wireframe(X, Y,U.detach().numpy().reshape(100,100))\n",
    "#ax.plot_wireframe(X, Y, Z, color = 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
